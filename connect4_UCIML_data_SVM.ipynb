{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "connect_4 = fetch_ucirepo(id=26) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = connect_4.data.features \n",
    "y = connect_4.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To begin processing the data, we needed to translate the values of x/o/b (game pieces or (b)lank/empty spots)\n",
    "and win/loss/draw into numbers that our SVMs could better work with. Once done with that, they are transformed\n",
    "into arrays for easy processing because out of the box SVMs are not set up to work with pandas dataframes.\n",
    "'''\n",
    "\n",
    "X_encode_map = {'x':1, 'o': 2, 'b': 0}\n",
    "X_encoded_dataframes = X.replace(X_encode_map)\n",
    "X_encoded = X_encoded_dataframes.values\n",
    "\n",
    "y_encode_map = {\"win\": 1, \"loss\": 2, \"draw\": 0}\n",
    "y_encoded_dataframes = y.replace({\"class\": y_encode_map})\n",
    "y_encoded = y_encoded_dataframes['class'].values\n",
    "\n",
    "'''\n",
    "Due to the size of the data (approximately 67.5k instances), we quickly realized we would need to find a way to speed\n",
    "up data processing if we wanted to do a thorough investigation of different hyperparameter settings without spending\n",
    "hours on every configuration. We will discuss this in more detail in several relevant sections below. To start,\n",
    "the code cell below this one has the PCA code necessary to determine the amount of principle components we can reduce\n",
    "to while still maintaing the majority of our data's information. The lines of code below this comment utilize the PCA\n",
    "findings in combination with an additional train_test_split that discards a portion of our data to give us vastly\n",
    "improved testing times to determine which hyperparameters are even reasonable to investigate more deeply.\n",
    "'''\n",
    "\n",
    "# X_encoded_reduced, X_discard, y_encoded_reduced, y_discard = train_test_split(\n",
    "#     X_encoded, y_encoded, test_size= 1/2, random_state=5)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_std_encoded_reduced = scaler.fit_transform(X_encoded_reduced)\n",
    "\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=5)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_std_encoded_reduced, y_encoded_reduced, test_size=0.3, random_state=5)\n",
    "\n",
    "# pca = PCA(n_components=24).fit(X_train)\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "#No scaling necessary due to the nature of the data??\n",
    "#TEST SIZE THE LAST VALUE TO EXPERIMENT WITH - .4 feels overboard. Feels like as low as 10% could work.\n",
    "#Keeping a set of test data out throughout the process to only use at the end?\n",
    "#Kfold of 3 actually very slightly reduced accuracy - theory: could be due to closely related models (say only one or two moves difference) being split across the folds.\n",
    "#   These closely related models could be crucial for helping the SVM to understand what particular small differences can have on the outcome.\n",
    "#Kfold - Taking a test set out at the very beginning then testing with that after cross validation?\n",
    "\n",
    "'''\n",
    "The few lines of code below are what we used when we wanted to test hyperparameters with the full dataset.\n",
    "'''\n",
    "\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(\n",
    "    X_encoded, y_encoded, test_size=0.3, random_state=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "# pca = PCA(n_components=42)\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# # Initialize variables to store feature counts for desired explained variance thresholds\n",
    "# feature_counts = {'80%': None, '85%': None, '90%': None}\n",
    "# total_variance = 0\n",
    "\n",
    "# # Calculate the cumulative explained variance ratio\n",
    "# for i, ratio in enumerate(explained_variance_ratio):\n",
    "#     total_variance += ratio\n",
    "#     for threshold in feature_counts:\n",
    "#         if feature_counts[threshold] is None and total_variance >= float(threshold.strip('%')) / 100:\n",
    "#             feature_counts[threshold] = i + 1  # Add 1 to convert from 0-based index to count of features\n",
    "#             break\n",
    "\n",
    "# # Print feature counts for different thresholds\n",
    "# print(\"Number of features needed to maintain:\")\n",
    "# for threshold, count in feature_counts.items():\n",
    "#     print(f\"{threshold} of the original information:\", count)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Explained variance by each component:\", pca.explained_variance_ratio_)\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.bar(range(1, 43), pca.explained_variance_ratio_, alpha=0.5, align='center', label='Individual explained variance')\n",
    "# plt.step(range(1, 43), np.cumsum(pca.explained_variance_ratio_), where='mid', label='Cumulative explained variance')\n",
    "# plt.ylabel('Explained variance ratio')\n",
    "# plt.xlabel('Principal components')\n",
    "# plt.legend(loc='best')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# components_df = pd.DataFrame(pca.components_, columns=X_encoded_dataframes.columns, index=['PC-' + str(i) for i in range(1, 43)])\n",
    "# print(components_df)\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# avg_feature_contributions = components_df.iloc[:42].mean()  # Calculate the average feature contributions for the first 21 principal components\n",
    "# avg_feature_contributions.plot(kind='bar')\n",
    "# plt.title('Average feature contributions for the first 42 principal components')\n",
    "# plt.xlabel('Original features')\n",
    "# plt.ylabel('Average feature contribution')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy with PCA: 0.6570949279652655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npoly arguments C and coef0\\nC - regularization parameters trade off between maximizing margin and minimizing classification error\\nsmall C - to softer margin, more points misclassified, but potentially more generalizable\\nlarge C - hard margin, wants most points correct, potentially overfits\\nShould be chosen through cross-validation or grid search\\n\\ncoef0 - the \"+ b\" independent term.\\nsmall coef0 - smoother decision boundary. Mitigates overfitting but doesn\\'t capture complexity as well.\\nlarge coef0 - decision boundary more sensitive to variations in input features. Can potentially better\\ncapture complexity but potentially overfits/doesn\\'t generalize well\\n\\nShould we try PCA? Maybe the middle columns are big determining factors? Answer = yes. Using \\n\\nTook 20 mins originally to do a GridSearch of reasonable hyperparameter variations using only 1/20th\\nthe size of the original date. Due to exponential growth of computation needed for both higher\\ndegree polynomials and all the pairwise comparisons that rbfs do, I knew that I needed to cut the\\ntest\\n\\n\\n\\nWhen I am writing about why I chose to go with using PCA to preprocess the data, is it reasonable for me to compare PCAs directly to the original components (individual board spaces) like so:\\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracies_linear = []\n",
    "#RENAME TO ACCURACY UNLESS YOU START AVERAGING ACCURACIES\n",
    "#models = ['linear', ]\n",
    "\n",
    "# model_linear = svm.SVC(kernel='linear')\n",
    "# model_linear.fit(X_train, y_train)\n",
    "# y_pred_linear = model_linear.predict(X_test)\n",
    "\n",
    "#Linear ultimately not a good choice for a classification with 3 classes. Could mitigate these factors by setting up a\n",
    "#One versus Rest model or train the model for every pair of classes compared to one another but our project is\n",
    "#More focused on comparing SVM to NN so we opted for models that can handle the complexity of C4\n",
    "#right out of the box.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the n_samples and n_features below are still a bit unclear to me, they just make an error go away\n",
    "# accuracies_linear = []\n",
    "n_samples, n_features = X_train.shape\n",
    "# svc_linear = LinearSVC(random_state=5, dual=n_samples > n_features)\n",
    "# svc_linear.fit(X_train, y_train)\n",
    "# y_pred_linear = svc_linear.predict(X_test)\n",
    "# accuracies_linear.append(np.mean(y_pred_linear == y_test))\n",
    "\n",
    "# print(accuracies_linear)\n",
    "\n",
    "#bad linear accuracy is 0.65674955595 and consistently takes over a minute to compute\n",
    "#good linear accuracy is 0.65714426682 and consistently takes about 5 seconds.\n",
    "#This may be because svm.SVC is using a more generalized library whereas the LinearSVC is\n",
    "#specifically for linear SVCs and therefore is likely highly optimized for those scenarios.\n",
    "\n",
    "\n",
    "\n",
    "n_samples, n_features = X_train_pca.shape  # Notice we use the transformed data shape here\n",
    "svc_linear = LinearSVC(random_state=5, dual=n_samples > n_features)\n",
    "svc_linear.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict using the trained LinearSVC on the PCA-transformed test data\n",
    "y_pred_linear = svc_linear.predict(X_test_pca)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = np.mean(y_pred_linear == y_test)\n",
    "print(\"LinearSVC accuracy with PCA:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('pca', PCA(n_components=27)),\n",
    "#     ('svm', LinearSVC(random_state=5, dual=n_samples > n_features))\n",
    "# ])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Predict using the pipeline\n",
    "# y_pred_linear = pipeline.predict(X_test)\n",
    "\n",
    "# # Calculate and print accuracy\n",
    "# accuracy = np.mean(y_pred_linear == y_test)\n",
    "# print(\"Pipeline accuracy with PCA:\", accuracy)\n",
    "\n",
    "'''\n",
    "poly arguments C and coef0\n",
    "C - regularization parameters trade off between maximizing margin and minimizing classification error\n",
    "small C - to softer margin, more points misclassified, but potentially more generalizable\n",
    "large C - hard margin, wants most points correct, potentially overfits\n",
    "Should be chosen through cross-validation or grid search\n",
    "\n",
    "coef0 - the \"+ b\" independent term.\n",
    "small coef0 - smoother decision boundary. Mitigates overfitting but doesn't capture complexity as well.\n",
    "large coef0 - decision boundary more sensitive to variations in input features. Can potentially better\n",
    "capture complexity but potentially overfits/doesn't generalize well\n",
    "\n",
    "Should we try PCA? Maybe the middle columns are big determining factors? Answer = yes. Using \n",
    "\n",
    "Took 20 mins originally to do a GridSearch of reasonable hyperparameter variations using only 1/20th\n",
    "the size of the original data. Due to exponential growth of computation needed for both higher\n",
    "degree polynomials and all the pairwise comparisons that rbfs do, I knew that I needed to cut the\n",
    "testing time for parameters down a bit. I used PCA to find the amount of PCAs required to retain\n",
    "85% of the original information (27 PCAs). Considering the low stakes of a Connect 4 game, this\n",
    "15% information loss seemed like a reasonable compromise to speed up testing times. I ran a test\n",
    "on the 'poly' SVCs from degree 2-7 with all of the original data and then again using 27 principal components\n",
    "instead of the 42 original components. Using PCA resulted in tests taking roughly only 75-80% of\n",
    "the original amount of time while still retaining accuracies within .008 of the original data set.\n",
    "\n",
    "\n",
    "\n",
    "When I am writing about why I chose to go with using PCA to preprocess the data, is it reasonable for me to compare PCAs directly to the original components (individual board spaces) like so:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bad - primarily used for binary classification problems.\n",
    "accuracy_sigmoid = []\n",
    "svc_sigmoid = svm.SVC(kernel='sigmoid', gamma='auto', coef0=1)\n",
    "svc_sigmoid.fit(X_train, y_train)\n",
    "y_pred_sigmoid = svc_sigmoid.predict(X_test)\n",
    "accuracy_sigmoid.append(np.mean(y_pred_sigmoid == y_test))\n",
    "print(accuracy_sigmoid)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Sigmoid Kernel SVM Accuracy:\", accuracy_score(y_test, y_pred_sigmoid))\n",
    "print(classification_report(y_test, y_pred_sigmoid))\n",
    "# Will not continue with sigmoid as it is not a good choice for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 4 degree:  [0.8211959739490823]\n",
      "Elapsed time: 2m 24.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nInitial full scale data testing of these values showed degree of 5 was the most accurate.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Below \n",
    "# accuracies_polynomial = []\n",
    "\n",
    "# for degree in range(2,8):\n",
    "#     svc_poly = svm.SVC(kernel='poly', degree=degree, C=1.0, coef0=1)\n",
    "#     svc_poly.fit(X_train, y_train)\n",
    "#     y_pred_poly = svc_poly.predict(X_test)\n",
    "#     accuracies_polynomial.append(np.mean(y_pred_poly == y_test))\n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     minutes, seconds = divmod(elapsed_time, 60)\n",
    "#     print(f\"Accuracy for {degree} degree: \", accuracies_polynomial)\n",
    "#     print(f\"Elapsed time: {int(minutes)}m {seconds:.1f}s\")\n",
    "#     accuracies_polynomial = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BELOW IS PCA VERSION\n",
    "\n",
    "accuracies_polynomial = []\n",
    "\n",
    "for degree in range(4,5):\n",
    "    svc_poly = svm.SVC(kernel='poly', degree=degree, C=1.0, coef0=1)\n",
    "    svc_poly.fit(X_train_pca, y_train)\n",
    "    y_pred_poly = svc_poly.predict(X_test_pca)\n",
    "    accuracies_polynomial.append(np.mean(y_pred_poly == y_test))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    print(f\"Accuracy for {degree} degree: \", accuracies_polynomial)\n",
    "    print(f\"Elapsed time: {int(minutes)}m {seconds:.1f}s\")\n",
    "    accuracies_polynomial = []\n",
    "\n",
    "'''\n",
    "Initial full scale data testing of these values showed degree of 5 was the most accurate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rbf = []\n",
    "\n",
    "#Gaussian Radial Basis Function\n",
    "svc_rbf = svm.SVC(kernel='rbf', C=10.0, gamma=0.1)\n",
    "svc_rbf.fit(X_train_pca, y_train)\n",
    "y_pred_rbf = svc_rbf.predict(X_test_pca)\n",
    "accuracy_rbf.append(np.mean(y_pred_rbf == y_test))\n",
    "print(accuracy_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6485193951347796\n",
      "0.6467508296314504\n",
      "0.647337278106509\n"
     ]
    }
   ],
   "source": [
    "#BREAK BELOW WILL BE USING KFOLD 5, 10, 20 -------------------------------\n",
    "#SECTION BEYOND SHOULD COMBINE KFOLD 5, 10, 20 WITH GRIDSEARCHCV - could take a very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 3 folds:\n",
      "Elapsed time: 3m 18.5s\n",
      "Avg accuracy for 2 degrees: 0.7592622553471126\n",
      "Avg accuracy for 3 degrees: 0.7871339537714341\n",
      "Avg accuracy for 4 degrees: 0.7908136200858787\n",
      "Avg accuracy for 5 degrees: 0.7789290375035754\n",
      "Avg accuracy for 6 degrees: 0.7596851437568922\n",
      "Avg accuracy for 7 degrees: 0.7543561874235079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m degree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m     16\u001b[0m     svc_poly \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m, degree\u001b[38;5;241m=\u001b[39mdegree, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, coef0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     \u001b[43msvc_poly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     y_pred_poly \u001b[38;5;241m=\u001b[39m svc_poly\u001b[38;5;241m.\u001b[39mpredict(X_test_fold)\n\u001b[0;32m     19\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_pred_poly \u001b[38;5;241m==\u001b[39m y_test_fold)\n",
      "File \u001b[1;32mc:\\Users\\deth_\\miniconda3\\envs\\CS345\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deth_\\miniconda3\\envs\\CS345\\lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\deth_\\miniconda3\\envs\\CS345\\lib\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folds = [3,5]\n",
    "\n",
    "\n",
    "for fold_num in folds:\n",
    "    start_time = time.time()\n",
    "    kf = KFold(n_splits=fold_num, shuffle=True, random_state=5)\n",
    "    accuracies_polynomial = {2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "    for train_index, test_index in kf.split(X_train_pca): \n",
    "\n",
    "        X_train_fold = X_train_pca[train_index]\n",
    "        X_test_fold = X_train_pca[test_index]\n",
    "        y_train_fold = y_train[train_index]\n",
    "        y_test_fold = y_train[test_index]\n",
    "\n",
    "        for degree in range(2,8):\n",
    "            svc_poly = svm.SVC(kernel='poly', degree=degree, C=1.0, coef0=1)\n",
    "            svc_poly.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_poly = svc_poly.predict(X_test_fold)\n",
    "            accuracy = np.mean(y_pred_poly == y_test_fold)\n",
    "            accuracies_polynomial[degree].append(accuracy)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    print(f\"For {fold_num} folds:\")\n",
    "    print(f\"Elapsed time: {int(minutes)}m {seconds:.1f}s\")\n",
    "    for degree in accuracies_polynomial:\n",
    "        avg_accuracy = np.mean(accuracies_polynomial[degree])\n",
    "        print(f\"Avg accuracy for {degree} degrees: {avg_accuracy}\")\n",
    "        \n",
    "\n",
    "'''\n",
    "Use this section to talk about how much increase in time is taken in exchange for what\n",
    "increase in accuracy for kfold. We will then use this section to determine the cv\n",
    "for the GridSearchCV. Originally tried 5/10/15. Stopped it at 10 because 10 took 265%\n",
    "the amount of time that 5 did and only produced a 0.41% avg increase in accuracy\n",
    "across degrees 2-7 (raw increase was 0.32%). Very much not worth the time.\n",
    "\n",
    "Increase from 3 to 5\n",
    "246% more time\n",
    "0.87% increase to accuracy (raw increase was 0.67%)\n",
    "For such a low stakes game situation, the amount of time required for such minimal increase\n",
    "in accuracy is not worth it considering how much this time will compound in a GridSearchCV\n",
    "'''\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy with rbf with 5 folds: 0.6496971290817445\n",
      "Avg accuracy with rbf with 10 folds: 0.6497032640949555\n",
      "Avg accuracy with rbf with 20 folds: 0.6500035221189069\n"
     ]
    }
   ],
   "source": [
    "folds = [3,5]\n",
    "\n",
    "for fold_num in folds:\n",
    "    kf = KFold(n_splits=fold_num, shuffle=True, random_state=5)\n",
    "    accuracy_rbf = []\n",
    "    for train_index, test_index in kf.split(X_encoded_reduced): #CHANGE BACK TO NOT REDUCED\n",
    "\n",
    "        X_train = X_encoded_reduced[train_index]\n",
    "        X_test = X_encoded_reduced[test_index]\n",
    "        y_train = y_encoded_reduced[train_index]\n",
    "        y_test = y_encoded_reduced[test_index]\n",
    "\n",
    "        #Gaussian Radial Basis Function\n",
    "        svc_rbf = svm.SVC(kernel='rbf', C=1.0, gamma=\"auto\")\n",
    "        svc_rbf.fit(X_train, y_train)\n",
    "        y_pred_rbf = svc_rbf.predict(X_test)\n",
    "        accuracy_rbf.append(np.mean(y_pred_rbf == y_test))\n",
    "\n",
    "    avg_accuracy = np.mean(accuracy_rbf)\n",
    "    print(f\"Avg accuracy with rbf with {fold_num} folds: {avg_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPros of RBF kernel\\n-Projects data into infinite-dimensional space - useful for complex datasets\\n-One less hyperparameter than poly\\nCons\\n-too high a gamma value risks overfitting\\n-computationally expensive - involves all pairs of points in dataset.\\n\\nPros of poly kernel\\n-Interpretability?\\n-More configurable - could finer tune for best answer (also a con in a way)\\nCons\\n-higher degree polynomials can have overfitting\\n\\nComparison\\n-if data has complex localized patterns (boundary is highly irregular), RBF may be better.\\n- poly strongly relies on finding correct degree, rbfs can adapt to unknown complex pattersn but at the cost\\nof computational power.\\n- both computationally expensive but the polynomial will get increasingly more expensive as degree goes higher\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Gamma values\n",
    "High gamma - decision boundary heavily influenced by points close to it. This means the boundary\n",
    "can be a lot less smooth and can capture more nuance in the data but at the risk of overfitting\n",
    "Low gamma - Smoother decision boundary. Will take into account more points farther away. Can\n",
    "better generalize at risk of underfitting the data with too simplistic a model.\n",
    "\n",
    "Pros of RBF kernel\n",
    "-Projects data into infinite-dimensional space - useful for complex datasets\n",
    "-One less hyperparameter than poly\n",
    "Cons\n",
    "-too high a gamma value risks overfitting\n",
    "-computationally expensive - involves all pairs of points in dataset.\n",
    "\n",
    "Pros of poly kernel\n",
    "-Interpretability?\n",
    "-More configurable - could finer tune for best answer (also a con in a way)\n",
    "Cons\n",
    "-higher degree polynomials can have overfitting\n",
    "\n",
    "Comparison\n",
    "-if data has complex localized patterns (boundary is highly irregular), RBF may be better.\n",
    "- poly strongly relies on finding correct degree, rbfs can adapt to unknown complex pattersn but at the cost\n",
    "of computational power.\n",
    "- both computationally expensive but the polynomial will get increasingly more expensive as degree goes higher\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "poly_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'coef0': [0.0, 0.5, 1.0, 3.0, 10.0], \n",
    "    'degree': [2, 3, 4, 5, 6],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "# grid_search = GridSearchCV(svm.SVC(), param_grid, cv=3)\n",
    "# grid_search.fit(X_train_pca, y_train)\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# y_pred_best = best_model.predict(X_test_pca)\n",
    "# print(\"Best Kernel SVM Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "# print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "\n",
    "\n",
    "poly_results = []\n",
    "for params in ParameterGrid(poly_param_grid):\n",
    "    model = svm.SVC(**params)\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)\n",
    "    #line above gets the output to not blast UndefinedMetricWarnings.\n",
    "    poly_results.append({\n",
    "        'params': params,\n",
    "        'accuracy': accuracy,\n",
    "        'report': report\n",
    "    })\n",
    "\n",
    "# Sort results by accuracy in descending order\n",
    "poly_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "# Optionally, print results\n",
    "first_result = True\n",
    "index = 1\n",
    "for result in poly_results:\n",
    "    if first_result:\n",
    "        print(f\"Best parameters: {result['params']}\")\n",
    "        print(f\"Best Kernel SVM Accuracy: {result['accuracy']:.3f}\")\n",
    "        first_result = False\n",
    "        index += 1\n",
    "    else:\n",
    "        print(f\"The {index}nd/th best option\")\n",
    "        print(result['params'])\n",
    "        print(f\"{result['accuracy']:.3f}\")\n",
    "        index += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Ran the code above with 1/20th the total data set and it took 20 minutes. Considering the\n",
    "exponential increase in time it would take to process, needed to choose an efficient path\n",
    "for testing. Initial param grids were set up dumb - lots of redundant tests - Using GridSearchCV\n",
    "with cv=3 which greatly increases time as well. Realizes from data that a 3 kfold actually slightly\n",
    "reduced accuracy in 1/2 data 24 PCs run so switched from GridSearchCV to simply ParameterGrid.\n",
    "\n",
    "1/20th data 24 PCs no CV - All degree 7s in the last half of 120 combinations. As seen in note in Excel,\n",
    "smaller data sets typically favor smaller degrees but given 7 was still reasonably lower than even 6\n",
    "in the original data run and how badly it performed here, I felt safe in removing it as it nearly\n",
    "doubles the time required.\n",
    "\n",
    "Data below is using 1/2 the full data set with 24 PCs AND NO CROSS VALIDATION! Did not finish in 180 mins.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best Kernel SVM Accuracy: 0.831\n",
      "The 2nd/th best option\n",
      "{'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.829\n",
      "The 3nd/th best option\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.826\n",
      "The 4nd/th best option\n",
      "{'C': 100, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.821\n",
      "The 5nd/th best option\n",
      "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.816\n",
      "The 6nd/th best option\n",
      "{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.804\n",
      "The 7nd/th best option\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.801\n",
      "The 8nd/th best option\n",
      "{'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.801\n",
      "The 9nd/th best option\n",
      "{'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.791\n",
      "The 10nd/th best option\n",
      "{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.771\n",
      "The 11nd/th best option\n",
      "{'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "0.748\n",
      "The 12nd/th best option\n",
      "{'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nRan 1/2 data 24 PCs - In the top half of results (10 of 20), gamma of .5 shows up once at 10th place.\\nRemoved gamma .5 for speed in full data set search.  C val of .5 only present in places 13 and beyond.\\nRemoved C val of .5 for speed.\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_param_grid = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1], \n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "rbf_results = []\n",
    "\n",
    "for params in ParameterGrid(rbf_param_grid):\n",
    "    model = svm.SVC(**params)\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=1)\n",
    "    #line above gets the output to not blast UndefinedMetricWarnings.\n",
    "    rbf_results.append({\n",
    "        'params': params,\n",
    "        'accuracy': accuracy,\n",
    "        'report': report\n",
    "    })\n",
    "\n",
    "# Sort results by accuracy in descending order\n",
    "rbf_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "# Optionally, print results\n",
    "first_result = True\n",
    "index = 1\n",
    "for result in rbf_results:\n",
    "    if first_result:\n",
    "        print(f\"Best parameters: {result['params']}\")\n",
    "        print(f\"Best Kernel SVM Accuracy: {result['accuracy']:.3f}\")\n",
    "        first_result = False\n",
    "        index += 1\n",
    "    else:\n",
    "        print(f\"The {index}nd/th best option\")\n",
    "        print(result['params'])\n",
    "        print(f\"{result['accuracy']:.3f}\")\n",
    "        index += 1\n",
    "\n",
    "'''\n",
    "Ran 1/2 data 24 PCs - In the top half of results (10 of 20), gamma of .5 shows up once at 10th place.\n",
    "Removed gamma .5 for speed in full data set search.  C val of .5 only present in places 13 and beyond.\n",
    "Removed C val of .5 for speed.\n",
    "\n",
    "Data below is using the full data set no PCs BUT NO CROSS VALIDATION!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto: 0.041666666666666664\n",
      "Scale: [0.0313389  0.03276203 0.03670258 0.03692228 0.03734812 0.043479\n",
      " 0.08751912 0.09102104 0.09625117 0.10039322 0.11197217 0.12027572\n",
      " 0.12680769 0.14474259 0.15691405 0.16041014 0.19386078 0.21602958\n",
      " 0.23085219 0.28391465 0.28816793 0.32670557 0.35266103 0.45419528]\n",
      "Mean Scale Gamma: 0.15671861840380516\n"
     ]
    }
   ],
   "source": [
    "n_samples_all, n_features_all = X_train_pca.shape\n",
    "auto = 1 / n_features_all\n",
    "print(f\"Auto: {auto}\")\n",
    "scale = 1 / (n_features_all * np.var(X_train_pca, axis=0))\n",
    "print(f\"Scale: {scale}\")\n",
    "mean_scale_gamma = np.mean(scale)\n",
    "print(\"Mean Scale Gamma:\", mean_scale_gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS345",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
